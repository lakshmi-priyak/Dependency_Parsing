# -*- coding: utf-8 -*-
"""Copy of SLP Assignment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-ymJgxA6CwkRh6ZiQSpOzexTzNKNPvt-
"""

!pip install -U spacy
!python -m spacy download en_core_web_sm

import spacy
from spacy.tokens import Doc
from spacy.training import Example
import random

# Load a blank English model
nlp = spacy.blank("en")

# Add a dependency parser
if "parser" not in nlp.pipe_names:
    parser = nlp.add_pipe("parser", last=True)

# Define dependency labels
for label in ["nsubj", "ROOT", "dobj"]:
    parser.add_label(label)

TRAIN_DATA = [
    ("I love NLP.", {"words": ["I", "love", "NLP", "."], "heads": [1, 1, 1, 1], "deps": ["nsubj", "ROOT", "dobj", "punct"]}),
    ("She plays soccer.", {"words": ["She", "plays", "soccer", "."], "heads": [1, 1, 1, 1], "deps": ["nsubj", "ROOT", "dobj", "punct"]}),
    ("He studies AI.", {"words": ["He", "studies", "AI", "."], "heads": [1, 1, 1, 1], "deps": ["nsubj", "ROOT", "dobj", "punct"]}),
]

examples = []
for text, annotations in TRAIN_DATA:
    words = annotations["words"]
    heads = annotations["heads"]
    deps = annotations["deps"]

    # Create a blank Doc with tokenized words
    doc = Doc(nlp.vocab, words=words)

    # Convert to spaCy Example format (Corrected)
    example = Example.from_dict(doc, {"heads": heads, "deps": deps})

    examples.append(example)

# Initialize optimizer
optimizer = nlp.initialize()

# Train for 50 epochs
for epoch in range(50):
    random.shuffle(examples)
    losses = {}
    nlp.update(examples, drop=0.5, losses=losses)
    print(f"Epoch {epoch + 1}, Losses: {losses}")

nlp.to_disk("dependency_parser_model")
print("Model saved successfully!")

# Load the trained model
nlp_loaded = spacy.load("dependency_parser_model")

# Test sentence
test_text = "He studies AI."
doc = nlp_loaded(test_text)

# Print results
print("\nDependency Parsing Results:")
for token in doc:
    print(f"{token.text} --> {token.dep_} (Head: {doc[token.head.i].text})")

from sklearn.metrics import classification_report

# True and predicted dependency labels
true_labels = ["nsubj", "ROOT", "dobj", "punct"]
pred_labels = [token.dep_ for token in doc]

# Flatten the lists before evaluation
print("\nEvaluation Metrics:")
print(classification_report(true_labels, pred_labels))

!pip install streamlit

!streamlit run streamlit.py

!cd /path/to/your_project
!touch streamlit.py

ls

!streamlit run streamlit.py